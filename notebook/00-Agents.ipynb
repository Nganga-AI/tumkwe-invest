{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_ollama import ChatOllama  # type: ignore\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph_agentflow.single_step import stream_agent_responses\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from tumkwe_invest import tools\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"llama3.3\", temperature=0.7)\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Visualize the Graph (Optional)\n",
    "try:\n",
    "\timg_data = graph.get_graph().draw_png()\n",
    "\tdisplay(Image(img_data))\n",
    "except Exception as e:\n",
    "\t# This requires some extra dependencies (like graphviz) and is optional\n",
    "\tprint(f\"Graph visualization failed (requires graphviz): {e}\")\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user-thread-1\"}}\n",
    "flag = True\n",
    "while flag:\n",
    "    try:\n",
    "        user_input = input(\"Enter your query (or 'exit' to quit): \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            flag = False\n",
    "            break\n",
    "\n",
    "        # Stream the agent's response\n",
    "        for step in stream_agent_responses(graph, user_input, config):\n",
    "            message = step[\"messages\"][-1]\n",
    "            message.pretty_print()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
